{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "[<font color='#E8800A'>1 - Building a Decision Tree</font>](#first-bullet) <br>\n",
    "[<font color='#E8800A'>2 - Avoid Overfitting</font>](#first-bullet) <br>\n",
    "[<font color='#E8800A'>3 - Feature importance with Decision Trees</font>](#first-bullet) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = pd.read_csv(r'diabetes.csv')\n",
    "X = diabetes.iloc[:,:-1]\n",
    "y = diabetes.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model):\n",
    "    # apply kfold\n",
    "    kf = KFold(n_splits=10)\n",
    "    # create lists to store the results from the different models \n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    timer = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # start counting time\n",
    "        begin = time.perf_counter()\n",
    "        # fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # finish counting time\n",
    "        end = time.perf_counter()\n",
    "        # check the mean accuracy for the train\n",
    "        value_train = model.score(X_train, y_train)\n",
    "        # check the mean accuracy for the test\n",
    "        value_test = model.score(X_test,y_test)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "        timer.append(end-begin)\n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_train = round(np.mean(score_train),3)\n",
    "    avg_test = round(np.mean(score_test),3)\n",
    "    std_time = round(np.std(timer),2)\n",
    "    std_train = round(np.std(score_train),2)\n",
    "    std_test = round(np.std(score_test),2)\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n",
    "str(avg_test) + '+/-' + str(std_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in anaconda prompt: conda install python-graphviz\n",
    "#!pip install pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "def plot_tree(model):\n",
    "    dot_data = export_graphviz(model,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "                               filled=True)\n",
    "    pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    pydot_graph.set_size('\"20,20\"')\n",
    "    return graphviz.Source(pydot_graph.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(df, *args):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    for arg in args:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_test = avg_score(arg)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_test\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "\n",
    "# 1. Building a Decision Tree\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ - Create an instance of DecisionTreeClassifier with the default parameters and name it as __dt_gini__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gini = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ - Fit your data to the model __dt_gini__ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ __Predicted Values__ <br>\n",
    "a) Check the predicted values for the test dataset using the method __predict()__ in your model<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_gini.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Check the predicted class probabilities for the test dataset using the method __predict_proba()__ in your model<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = dt_gini.predict_proba(X_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Check the depth (__get_depth()__), the number of nodes (__.tree_.node_count__) and the number of leaves (__get_n_leaves()__) of the model __dt_gini__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The defined three has a depth of ' + str(dt_gini.get_depth()) + ', ' + str(dt_gini.tree_.node_count) + \n",
    "      ' nodes and a total of ' + str(dt_gini.get_n_leaves()) + ' leaves.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>criterion | </font>  <font color='#3a7f8f'>Changing the split criteria</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 'gini'`\n",
    "\n",
    "- A decision tree is split by using the impurity - a measure of homogeneity of the labels on the node.\n",
    "- There are two possibilities in sklearn:\n",
    "    - Gini Index - Gini Impurity measures the divergences between the probability distributions of the target attribute’s values and splits a node such that it gives the least amount of impurity.\n",
    "    - Entropy - Information gain uses the entropy measure as the impurity measure and splits a node such that it gives the most amount of information gain.\n",
    "    \n",
    "- In most cases, the choice of splitting criteria will not make much difference in the performance of the model. However, and according to the \"No free lunch Theorem\", each criterion is superior in some cases and inferior in others.\n",
    "- The main difference is that entropy might be a little slower to compute because it requires you to compute a logarithmic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ - Create an instance of DecisionTreeClassifier named as __dt_entropy__ and define the parameter __criterion='entropy'__, and fit the data to your model. Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_entropy = DecisionTreeClassifier(criterion = 'entropy').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['Gini','Entropy'])\n",
    "show_results(df,dt_gini, dt_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# 2. Avoiding Overfitting\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## Prepruning a tree\n",
    "\n",
    "[2.1. - The splitter](#splitter) <br>\n",
    "[2.2. - The maximum depth](#depth)<br>\n",
    "[2.3. - The minimum number of samples required to split](#samples)<br>\n",
    "[2.4. - The minimum samples in each leaf](#leaf)<br>\n",
    "[2.5. - The minimum weight fraction in each leaf](#weight)<br>\n",
    "[2.6. - The maximum number of features](#features)<br>\n",
    "[2.7. - The maximum number of leaf nodes](#nodes)<br>\n",
    "[2.8. - The minimum impurity decrease](#decrease)<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>splitter| </font>  <font color='#3a7f8f'>Changing the splitter</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 'best'`\n",
    "\n",
    "If random, it selects a random feature and a random split in each feature. \n",
    "- It's less computation intensive than calculating the optimal split of every feature at every leaf.\n",
    "- It should be less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__ - Create an instance of DecisionTreeClassifier named as __dt_random__ and define the parameter __splitter='random'__, and fit the data to your model. Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random = DecisionTreeClassifier(splitter = 'random').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['best','random'])\n",
    "show_results(df,dt_gini, dt_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>max_depth | </font>  <font color='#3a7f8f'>Changing the maximum depth of a tree</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 'None'`\n",
    "\n",
    "\n",
    "\n",
    "- If you don’t specify a depth for the tree, scikit-learn will expand the nodes until all leaves are pure (unless other parameters are defined)\n",
    "- The deeper you allow your tree to grow, the more complex your model will be. \n",
    "- __`High Depth`__ - This will increase the number of slipts and captures more information about the data. However, this is one of the major causes associated with overfitting, since your model will fit perfectly for the training data, and it will not be able to generalize well on test. \n",
    "- __`Low Depth`__ - This is one of the major causes associated with underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ - Create an instance of DecisionTreeClassifier named as __dt_depth2__ and define the parameter __max_depth=2__, and fit the data to your model. Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_depth2 = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['full','depth2'])\n",
    "show_results(df,dt_gini, dt_depth2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 8`__ - Use the package graphviz to visualize the Decision Tree just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_depth2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>min_samples_split |</font>  <font color='#3a7f8f'>Changing the minimum number of samples required to split an internal node</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 2`\n",
    "\n",
    "- An internal node can have further splits (on the other hand, leafs is a node without children)\n",
    "- It is used to control overfitting\n",
    "- __`High Values`__ - Prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "- __`Too high Values`__ - Can lead to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 9`__ - Create an instance of DecisionTreeClassifier named as __dt_min10__ and define the parameter __min_samples_split=10__, and fit the data to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min10 = DecisionTreeClassifier(min_samples_split = 10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 10`__ - Create an instance of DecisionTreeClassifier named as __dt_min500__ and define the parameter __min_samples_split=500__, and fit the data to your model. Check the results for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min500 = DecisionTreeClassifier(min_samples_split = 500).fit(X_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['dt_min10','dt_min500'])\n",
    "show_results(df, dt_min10, dt_min500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11`__ Plot the decision tree __dt_min500__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_min500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>min_samples_leaf |</font> <font color='#3a7f8f'>Changing the minimum number of samples required to be at a leaf node</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 1`\n",
    "\n",
    "- A leaf is a node without children\n",
    "- It is used to control overfitting, by defining that each leaf has more than one element\n",
    "- __`Small Values`__ - The tree will overfit\n",
    "- __`Too high Values`__ - Can lead to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ - Create an instance of DecisionTreeClassifier named as __dt_min_sam200__ and define the parameter __min_samples_split=200__, and fit the data to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_sam200 = DecisionTreeClassifier(min_samples_leaf = 200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ - Create an instance of DecisionTreeClassifier named as __dt_min_sam500__ and define the parameter __min_samples_split=500__, and fit the data to your model. Compare the results between the baseline model and the models created in step12 and step13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_sam500 = DecisionTreeClassifier(min_samples_leaf = 500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['dt_min_sam1','dt_min_sam200','dt_min_sam500'])\n",
    "show_results(df,dt_gini, dt_min_sam200, dt_min_sam500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 14`__ Plot the decision tree __dt_min_sam200__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_min_sam200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>min_weight_fraction_leaf |</font> <font color='#3a7f8f'>Changing the minimum number of samples required to be at a leaf node</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the fraction of the input samples required to be at a leaf node where weights are determined by sample_weight, this is a way to deal with class imbalance. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights for each class to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ - Create an instance of DecisionTreeClassifier named as __dt_min_weight__ and define the parameter __min_weight_fraction_leaf=0.15__, and fit the data to your model. Compare the results with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_weight = DecisionTreeClassifier(min_weight_fraction_leaf = 0.15).fit(X_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['dt_gini','dt_min_weight'])\n",
    "show_results(df,dt_gini, dt_min_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ Plot the decision tree __dt_min_weight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_min_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>max_features |</font> <font color='#3a7f8f'>Changing the number of features to consider when looking for the best split</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 'None'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is computationally heavy to look at all the features every single time, so you can just check some of them using the various max_features options\n",
    "- It also allows to minimize overfitting - by choosing a reduced number of features, we can increase the stability of the tree and reduce variance and overfitting\n",
    "\n",
    "There are several options (let's imagine we are dealing with 32 variables): <br>\n",
    "`int` - The defined value is the number of maximum features to be considered at each split<br>\n",
    "    - A value of 10 will consider 10 features\n",
    "`float` - The defined value will be multiplied by the number of features and those are considered to each split\n",
    "    - A value of 0.5 will consider 16 features\n",
    "`auto` - The number of features considered is equal to sqrt(total number of features)\n",
    "    - It will be considered 6 features\n",
    "`log2` - The number of features considered is equal to log2(total number of features)\n",
    "    - It will be considered 5 features\n",
    "`None`- The number of features considered is equal to the total number of features\n",
    "    - 32 variables will be considered\n",
    "    \n",
    "The option to select will depend on the number of features you have, the computational intensity you want to reduce or the amount of overfitting you have, so if you have a high computational cost or you have a lot of overfitting, you can try with “log2” and depending on what that produces, you can either bring it slightly up using sqrt or take it down further using a custom float value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 17`__ - Create the following instances of a DecisionTreeClassifier:\n",
    "- where __max_features = None__ and name it as __dt_none__ (The baseline model)\n",
    "- where __max_features = 2__ and name it as __dt_int__\n",
    "- where __max_features = 0.5__ and name it as __dt_float__\n",
    "- where __max_features = 'auto'__ and name it as __dt_auto__\n",
    "- where __max_features = 'log2'__ and name it as __dt_log2__\n",
    "\n",
    "\n",
    "Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_none = DecisionTreeClassifier(max_features = None).fit(X_train, y_train)\n",
    "dt_int = DecisionTreeClassifier(max_features = 2).fit(X_train, y_train)\n",
    "dt_float = DecisionTreeClassifier(max_features = 0.5).fit(X_train, y_train)\n",
    "dt_auto = DecisionTreeClassifier(max_features = 'auto').fit(X_train, y_train)\n",
    "dt_log2 = DecisionTreeClassifier(max_features = 'log2').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['None (Baseline)','Int','Float','Auto','Log2'])\n",
    "show_results(df,dt_none, dt_int, dt_float, dt_auto, dt_log2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 18`__ - Create the following instances of a DecisionTreeClassifier:\n",
    "- where __max_features = 2__ and __max_depth = 2__ and name it as __dt_int2__\n",
    "- where __max_features = 2__ and __max_depth = 2__ and name it as __dt_int3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_int2 = DecisionTreeClassifier(max_features = 2, max_depth = 2).fit(X_train, y_train)\n",
    "dt_int3 = DecisionTreeClassifier(max_features = 2, max_depth = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 19`__ Plot the decision tree __dt_int2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_int2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 20`__ Plot the decision tree __dt_int3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_int3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>max_leaf_nodes |</font> <font color='#3a7f8f'>Define the total number of leaf nodes</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = 'None'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 21`__ - Create an instance of DecisionTreeClassifier named as __dt_maxleaf5__ and define the parameter __max_leaf_nodes=5__, and fit the data to your model. Compare the results with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_maxleaf5 = DecisionTreeClassifier(max_leaf_nodes = 5).fit(X_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['Baseline','dt_maxleaf5'])\n",
    "show_results(df,dt_gini, dt_maxleaf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 22`__ Plot the decision tree __dt_maxleaf5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_maxleaf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#E8800A'>min_impurity_decrease |</font> <font color='#3a7f8f'>Decide if a node will be split according to the decrease of impurity</font> <a class=\"anchor\" id=\"first-bullet\"></a><br><br>`default = '0.'`\n",
    "\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 23`__ - Create an instance of DecisionTreeClassifier named as __dt_impurity02__ and define the parameter __min_impurity_decrease=0.02__, and fit the data to your model. Compare the results with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_impurity02 = DecisionTreeClassifier(min_impurity_decrease=0.02).fit(X_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['Baseline','dt_impurity04'])\n",
    "show_results(df,dt_gini, dt_impurity04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 24`__ Plot the decision tree __dt_impurity02__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt_impurity02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# 2. Use a decision tree to evaluate feature importance\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 25`__ Calculate the feature importance using the split criteria 'Gini' and 'Entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_importance = DecisionTreeClassifier().fit(X_train, y_train).feature_importances_\n",
    "entropy_importance = DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train).feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 26`__ Plot the feature importances for both criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "zippy = pd.DataFrame(zip(gini_importance, entropy_importance), columns = ['gini','entropy'])\n",
    "zippy['col'] = X_train.columns\n",
    "tidy = zippy.melt(id_vars='col').rename(columns=str.title)\n",
    "tidy.sort_values(['Value'], ascending = False, inplace = True)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(y='Col', x='Value', hue='Variable', data=tidy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
