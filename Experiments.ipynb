{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import layers,models,optimizers, applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset\n",
    "[Download](https://liveeduisegiunl-my.sharepoint.com/:u:/g/personal/m20200597_novaims_unl_pt/EWfJRtW0BRRNoOmYzb6FviQB3_HP_N-rsnPcxm4J7fxEBQ?e=8Mh1xt)\n",
    "\n",
    "Download file and unzip in the base_dir directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSamplePercentage = 70\n",
    "RandomSeed = 42\n",
    "base_dir = \"C:\\\\Users\\\\flopes\\\\Desktop\\\\CovidChestXRay\" # Change to personal folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageBank = \"ImageBank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageBankPath = os.path.join(base_dir, ImageBank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_images_path = os.path.join(ImageBankPath, 'COVID')\n",
    "noncovid_images_path = os.path.join(ImageBankPath, 'NOCOVID')\n",
    "\n",
    "len_covid = int(len(os.listdir(covid_images_path)) * TrainSamplePercentage / 100)\n",
    "len_no_covid = int(len(os.listdir(noncovid_images_path)) * TrainSamplePercentage / 100)\n",
    "\n",
    "print(f\"COVID - Train Sample Size : {len_covid}\")\n",
    "print(f\"NO COVID - Train Sample Size : {len_no_covid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  os.path.join(base_dir, 'train')\n",
    "validation_path = os.path.join(base_dir, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTrainAndVal(source_COVID, source_NOCOVID,dest_train,dest_val,TrainSamplePercentage, RandomSeed):\n",
    "    create_dir(dest_train)\n",
    "    create_dir(dest_val)\n",
    "    \n",
    "    # Create class folder in train\n",
    "    covid_train_path =  os.path.join(dest_train, 'COVID')\n",
    "    create_dir(covid_train_path)\n",
    "    no_covid_train_path = os.path.join(dest_train, 'NOCOVID')\n",
    "    create_dir(no_covid_train_path)\n",
    "    \n",
    "    # Create class folder in val\n",
    "    covid_val_path =  os.path.join(dest_val, 'COVID')\n",
    "    create_dir(covid_val_path)\n",
    "    no_covid_val_path = os.path.join(dest_val, 'NOCOVID')\n",
    "    create_dir(no_covid_val_path)\n",
    "    \n",
    "    \n",
    "    ImageSplit = train_test_split(os.listdir(source_COVID), test_size=(100-TrainSamplePercentage)/100.0, random_state=RandomSeed)\n",
    "    \n",
    "    for image in ImageSplit[0]: # train\n",
    "        src = os.path.join(source_COVID, image)\n",
    "        dest = os.path.join(covid_train_path, image)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    for image in ImageSplit[1]: # val\n",
    "        src = os.path.join(source_COVID, image)\n",
    "        dest = os.path.join(covid_val_path, image)\n",
    "        shutil.copyfile(src, dest)\n",
    "\n",
    "        \n",
    "    ImageSplit = train_test_split(os.listdir(source_NOCOVID), test_size=(100-TrainSamplePercentage)/100.0, random_state=RandomSeed)\n",
    "    \n",
    "    for image in ImageSplit[0]: # train\n",
    "        src = os.path.join(source_NOCOVID, image)\n",
    "        dest = os.path.join(no_covid_train_path, image)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    for image in ImageSplit[1]: # val\n",
    "        src = os.path.join(source_NOCOVID, image)\n",
    "        dest = os.path.join(no_covid_val_path, image)\n",
    "        shutil.copyfile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateTrainAndVal(covid_images_path, \n",
    "                  noncovid_images_path,\n",
    "                  train_path,\n",
    "                  validation_path,\n",
    "                  TrainSamplePercentage, \n",
    "                  RandomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras(model):\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(parameters, experiments):\n",
    "    print(\"Running : # Run \", parameters['# Run'])\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32,parameters['filter'],activation=parameters['activation']\n",
    "                            , input_shape=parameters['input_shape'],padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64,parameters['filter'],activation=parameters['activation'],padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "    model.add(layers.Conv2D(128,parameters['filter'],activation=parameters['activation'], padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "    model.add(layers.Conv2D(128,parameters['filter'],activation=parameters['activation'], padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "    \n",
    "    if parameters['layers_to_add'] > 0:\n",
    "        for i in range(parameters['layers_to_add']):\n",
    "            model.add(layers.Conv2D(128,parameters['filter'],activation=parameters['activation'], padding='same'))\n",
    "            model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation=parameters['activation']))\n",
    "    model.add(layers.Dense(1, activation=parameters['activation_output']))\n",
    "    \n",
    "    if parameters['optimizer'] == 'RMSprop':\n",
    "        optimizer = optimizers.RMSprop(lr=parameters['learning_rate'])\n",
    "    else:\n",
    "        optimizer =  optimizers.Adam(learning_rate=parameters['learning_rate'])\n",
    "\n",
    "    model.compile(loss=parameters['loss_function'], optimizer=optimizer,metrics=['acc','AUC'])\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255, rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(train_path, target_size=parameters['target_size'],\n",
    "                                                   batch_size=parameters['batch_size'],\n",
    "                                                   class_mode='binary')\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(validation_path, target_size=parameters['target_size'],\n",
    "                                                   batch_size=parameters['batch_size'],\n",
    "                                                   class_mode='binary')\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    step_size_train = train_generator.n//train_generator.batch_size\n",
    "    step_size_val = validation_generator.n//validation_generator.batch_size    \n",
    "    \n",
    "    history = model.fit(train_generator,\n",
    "                      steps_per_epoch = step_size_train,\n",
    "                      validation_data = validation_generator,\n",
    "                      epochs = parameters['epochs'],\n",
    "                      validation_steps = step_size_val\n",
    "                     )\n",
    "    \n",
    "    \n",
    "    final_time = (time.time() - start_time)\n",
    "    parameters['model_name'] = \"Custom\"\n",
    "    parameters['execution_time'] = final_time / 60\n",
    "    parameters['accuracy'] = history.history['acc'][-1]\n",
    "    parameters['auc'] = history.history['auc'][-1]\n",
    "    parameters['loss'] = history.history['loss'][-1]\n",
    "    parameters['val_loss'] = history.history['val_loss'][-1]\n",
    "    parameters['val_acc'] = history.history['val_acc'][-1]\n",
    "    parameters['val_auc'] = history.history['val_auc'][-1]\n",
    "    experiments.append(parameters)\n",
    "    \n",
    "    del history\n",
    "    reset_keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "parameters_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(150,150,3), 'loss_function':'binary_crossentropy','target_size':(150,150)\n",
    "              , 'epochs': 20, 'learning_rate': 1e-4, 'comments': 'Inial Parameters', 'optimizer': 'RMSprop', 'layers_to_add': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the Image Size\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256)\n",
    "              , 'epochs': 20, 'learning_rate': 1e-4, 'comments': 'Change the image size', 'optimizer': 'RMSprop', 'layers_to_add': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the Learning Rate\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256)\n",
    "              , 'epochs': 20, 'learning_rate': 0.01, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the Learning Rate #1\"})\n",
    "\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the Learning Rate #2\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the Filter Size\n",
    "parameters_list.append({'batch_size': 50,'filter': (5,5),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the Filter Size #1\"})\n",
    "\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the Filter Size #2\"})\n",
    "\n",
    "parameters_list.append( {'batch_size': 50,'filter': (12,12),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the Filter Size #3\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increase the number of epochs\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256)\n",
    "              , 'epochs': 30, 'learning_rate': 1e-4, 'comments': 'Change the number of epochs #1', 'optimizer': 'RMSprop', 'layers_to_add': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CHOOSE THIS ONE\n",
    "parameters_list.append({'batch_size': 50,'filter': (3,3),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256)\n",
    "              , 'epochs': 40, 'learning_rate': 1e-4, 'comments': 'Change the number of epochs #2', 'optimizer': 'RMSprop', 'layers_to_add': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimizer\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'adam', 'layers_to_add': 0,'comments': \"Testing Adam optimizer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding More Layers\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'adam', 'layers_to_add': 2,'comments': \"Testing Adam optimizer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the activation function\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'softmax',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'adam', 'layers_to_add': 2,'comments': \"Change the activation function #1\"})\n",
    "\n",
    "#Adam optimizer\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'softmax',\n",
    "              'input_shape':(256,256,3), 'loss_function':'binary_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'adam', 'layers_to_add': 0,'comments': \"Change the activation function #2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical cross entropy\n",
    "parameters_list.append({'batch_size': 50,'filter': (5,5),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'categorical_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'RMSprop', 'layers_to_add': 0,'comments': \"Changing the loss function #1\"})\n",
    "\n",
    "parameters_list.append( {'batch_size': 50,'filter': (8,8),'activation': 'relu', 'activation_output': 'sigmoid',\n",
    "              'input_shape':(256,256,3), 'loss_function':'categorical_crossentropy','target_size':(256,256), 'epochs': 20\n",
    "              , 'learning_rate': 0.001, 'optimizer': 'adam', 'layers_to_add': 0,'comments': \"Changing the loss function #2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for parameters in parameters_list:\n",
    "    parameters['# Run'] = i\n",
    "    i+=1\n",
    "    run_tests(parameters,experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferLearning(basemodel,imageprocessor,parameters,experiments):\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    inputs = keras.Input(shape=parameters['input_shape'])\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(2,activation=parameters['activation_output'])(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=parameters['learning_rate']),\n",
    "                  loss=parameters['loss_function'],\n",
    "                  metrics=['acc','AUC'])\n",
    "    \n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=imageprocessor)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=imageprocessor)\n",
    "\n",
    "    train_generator=train_datagen.flow_from_directory(train_path,\n",
    "                                                     color_mode='rgb',\n",
    "                                                     batch_size=parameters['batch_size'],\n",
    "                                                     class_mode='categorical',\n",
    "                                                     shuffle=True)\n",
    "\n",
    "    val_generator=val_datagen.flow_from_directory(validation_path, \n",
    "                                                     color_mode='rgb',\n",
    "                                                     batch_size=parameters['batch_size'],\n",
    "                                                     class_mode='categorical',\n",
    "                                                     shuffle=True)\n",
    "    \n",
    "    step_size_train = train_generator.n//train_generator.batch_size\n",
    "    step_size_val = val_generator.n//val_generator.batch_size\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_generator,\n",
    "                          steps_per_epoch = step_size_train,\n",
    "                          validation_data = val_generator,\n",
    "#                           epochs = 1,\n",
    "                          epochs = parameters['epochs'],\n",
    "                          validation_steps = step_size_val\n",
    "                         )\n",
    "    \n",
    "    final_time = (time.time() - start_time)\n",
    "\n",
    "    parameters['execution_time'] = final_time / 60\n",
    "    parameters['accuracy'] = history.history['acc'][-1]\n",
    "    parameters['auc'] = history.history['auc'][-1]\n",
    "    parameters['loss'] = history.history['loss'][-1]\n",
    "    parameters['val_loss'] = history.history['val_loss'][-1]\n",
    "    parameters['val_acc'] = history.history['val_acc'][-1]\n",
    "    parameters['val_auc'] = history.history['val_auc'][-1]\n",
    "    experiments.append(parameters)\n",
    "    \n",
    "    del history\n",
    "    reset_keras(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCeption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.Xception(weights='imagenet',\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    include_top=False)\n",
    "\n",
    "imageprocessor = applications.xception.preprocess_input\n",
    "\n",
    "tf_parameters = {\n",
    "    '# Run' : len(experiments) + 1,\n",
    "    'model_name': 'XCeption',\n",
    "    'activation_output': 'softmax',\n",
    "    'filter': 'NA',\n",
    "    'layers_to_add': 'NA',\n",
    "    'activation': 'NA',\n",
    "    'input_shape':(224, 224, 3),\n",
    "    'epochs': 30,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'loss_function' : 'categorical_crossentropy'\n",
    "    \n",
    "}\n",
    "\n",
    "transferLearning(base_model,imageprocessor,tf_parameters,experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.InceptionV3(weights='imagenet',\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    include_top=False)\n",
    "\n",
    "imageprocessor = applications.inception_v3.preprocess_input\n",
    "\n",
    "tf_parameters = {\n",
    "    '# Run' : len(experiments) + 1,\n",
    "    'model_name': 'InceptionV3',\n",
    "    'activation_output': 'softmax',\n",
    "    'filter': 'NA',\n",
    "    'layers_to_add': 'NA',\n",
    "    'activation': 'NA',\n",
    "    'input_shape':(224, 224, 3),\n",
    "    'epochs': 30,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'loss_function' : 'categorical_crossentropy'\n",
    "    \n",
    "}\n",
    "\n",
    "transferLearning(base_model,imageprocessor,tf_parameters,experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.DenseNet169(weights='imagenet',\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    include_top=False)\n",
    "\n",
    "imageprocessor = applications.densenet.preprocess_input\n",
    "\n",
    "tf_parameters = {\n",
    "    '# Run' : len(experiments) + 1,\n",
    "    'model_name': 'DenseNet169',\n",
    "    'activation_output': 'softmax',\n",
    "    'filter': 'NA',\n",
    "    'layers_to_add': 'NA',\n",
    "    'activation': 'NA',\n",
    "    'input_shape':(224, 224, 3),\n",
    "    'epochs': 30,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'loss_function' : 'categorical_crossentropy'\n",
    "    \n",
    "}\n",
    "\n",
    "transferLearning(base_model,imageprocessor,tf_parameters,experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG16(weights='imagenet',\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    include_top=False)\n",
    "\n",
    "imageprocessor = applications.vgg16.preprocess_input\n",
    "\n",
    "tf_parameters = {\n",
    "    '# Run' : len(experiments) + 1,\n",
    "    'model_name': 'VGG16',\n",
    "    'activation_output': 'softmax',\n",
    "    'filter': 'NA',\n",
    "    'layers_to_add': 'NA',\n",
    "    'activation': 'NA',\n",
    "    'input_shape':(224, 224, 3),\n",
    "    'epochs': 30,\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'loss_function' : 'categorical_crossentropy'\n",
    "    \n",
    "}\n",
    "\n",
    "transferLearning(base_model,imageprocessor,tf_parameters,experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG16(weights='imagenet',\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    include_top=False)\n",
    "\n",
    "imageprocessor = applications.vgg16.preprocess_input\n",
    "\n",
    "tf_parameters = {\n",
    "    '# Run' : len(experiments) + 1,\n",
    "    'model_name': 'VGG16',\n",
    "    'activation_output': 'softmax',\n",
    "    'filter': 'NA',\n",
    "    'layers_to_add': 'NA',\n",
    "    'activation': 'NA',\n",
    "    'input_shape':(224, 224, 3),\n",
    "    'epochs': 100,\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'loss_function' : 'categorical_crossentropy'\n",
    "    \n",
    "}\n",
    "\n",
    "transferLearning(base_model,imageprocessor,tf_parameters,experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
